{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# CNV association mapping simulation and analysis workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Pipeline command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run 20190717_workflow.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  genome_partition\n",
      "  susie\n",
      "  varbvs\n",
      "  fisher\n",
      "  mcmc\n",
      "  mcmc_multichain\n",
      "  sier\n",
      "  hybrid\n",
      "  get_hist\n",
      "  varbvs_wg\n",
      "  simulate\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd output (as path)\n",
      "  --genotype-file data/deletion.X.gz (as path)\n",
      "  --phenotype-file data/deletion.y.gz (as path)\n",
      "                        phenotype\n",
      "  --name ''\n",
      "  --blocks-file . (as path)\n",
      "  --iteration 5000 (as int)\n",
      "                        MCMC number of iterations\n",
      "  --tune-prop 0.25 (as float)\n",
      "                        MCMC ...\n",
      "  --target-accept 0.98 (as float)\n",
      "                        MCMC ...\n",
      "  --n-chain 10 (as int)\n",
      "                        MCMC ...\n",
      "  --n-core 1 (as int)\n",
      "                        MCMC ... For some reason on RCC cluster more than 1\n",
      "                        cores will not work (stuck)\n",
      "  --[no-]reparameterize (default to False)\n",
      "                        MCMC ...\n",
      "  --prevalence 0.05 (as float)\n",
      "                        alpha = log(p/1-p) is uniform lower bound: p =\n",
      "                        prevalence, when prevalence = 0.05, alpha = -2.94 upper\n",
      "                        bound: p = case proportion, when case = control, alpha =\n",
      "                        0 MCMC ...\n",
      "  --mcmc-seed 999 (as int)\n",
      "                        MCMC ...\n",
      "  --hyperparam-file . (as path)\n",
      "                        Hyper-parameters for MCMC and for Single Effect\n",
      "                        Regression\n",
      "  --L 10 (as int)\n",
      "                        SuSiE number of effects\n",
      "  --varbvs-wg-pip . (as path)\n",
      "                        Whole genome PIPs obtained by varbvs using `varbvs_wg`\n",
      "                        pipeline, used for hybrid pipeline\n",
      "  --mcmc-walltime '2.5h'\n",
      "                        cluster job configurations\n",
      "  --job-size 80 (as int)\n",
      "\n",
      "Sections\n",
      "  genome_partition:\n",
      "    Workflow Options:\n",
      "      --input-file VAL (as str, required)\n",
      "                        For simulation: get real deletion/duplication CNV data\n",
      "                        and its block n_gene_in_block: get_hist: 1, simulate:\n",
      "                        20~50, analyze: 1\n",
      "      --output-files VAL VAL ... (as type, required)\n",
      "                        output contain 3 files: 1) input data removing columns\n",
      "                        with all zeros, 2) file containing block start and end\n",
      "                        matching index in 1), 3) block start and end without\n",
      "                        reindex\n",
      "      --n-gene-in-block VAL (as int, required)\n",
      "                        minimum number of genes in a block for copy model set it\n",
      "                        to 1 to use \"natural blocks\" from input data\n",
      "      --col-index VAL (required)\n",
      "                        col_index=None: no row names, col_index=0: use first\n",
      "                        column as row names\n",
      "  susie_1, varbvs_1, fisher_1, mcmc_1, mcmc_multichain_1, sier_1, hybrid_1,\n",
      "                        get_hist_1:\n",
      "  fisher_2:\n",
      "  susie_2, varbvs_2, mcmc_2, mcmc_multichain_2, sier_2, hybrid_2:\n",
      "  hybrid_3:\n",
      "  mcmc_multichain_3:\n",
      "  mcmc_3:\n",
      "  sier_3:\n",
      "    Workflow Options:\n",
      "      --expected-effects -9 (as int)\n",
      "  mcmc_4, mcmc_multichain_4, sier_4, hybrid_4:\n",
      "  susie_3:\n",
      "    Workflow Options:\n",
      "      --estimate-prior-method simple\n",
      "      --check-null-threshold 0.1 (as float)\n",
      "  susie_4, varbvs_4:\n",
      "  varbvs_3:\n",
      "  varbvs_wg:\n",
      "    Workflow Options:\n",
      "      --maximum-prior-inclusion 0.0 (as float)\n",
      "  get_hist_2:\n",
      "  simulate_1:\n",
      "    Workflow Options:\n",
      "      --n-gene-in-block 30 (as int)\n",
      "  simulate_2:\n",
      "  simulate_3:\n",
      "    Workflow Options:\n",
      "      --shape 1.4 (as float)\n",
      "      --scale 0.6 (as float)\n",
      "      --beta-method normal\n",
      "      --pi0 0.95 (as float)\n",
      "      --seed 999 (as int)\n",
      "  simulate_4:\n",
      "    Workflow Options:\n",
      "      --sample-size 100000 (as int)\n",
      "      --n-batch 200 (as int)\n",
      "  simulate_5:\n",
      "  simulate_6:\n"
     ]
    }
   ],
   "source": [
    "sos run 20190717_workflow.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal examples\n",
    "\n",
    "### Simulate data\n",
    "```\n",
    "sos run 20190717_workflow.ipynb simulate \\\n",
    "    --name simulation_0525 \\\n",
    "    --genotype-file /home/min/GIT/cnv-gene-mapping/data/deletion.X.gz \\\n",
    "    --cwd output \\\n",
    "    --sample-size 500 \\\n",
    "    --n-batch 10\n",
    "```\n",
    "\n",
    "### Whole genome analysis using `varbvs`\n",
    "```\n",
    "sos run 20190717_workflow.ipynb varbvs_wg \\\n",
    "    --name varbvs_0525 --cwd output \\\n",
    "    --genotype-file output/deletion.X_b30.simulation_0525.samples.X.gz \\\n",
    "    --phenotype-file output/deletion.X_b30.simulation_0525.samples.y.gz\n",
    "```\n",
    "\n",
    "### Fisher's exact test per gene\n",
    "```\n",
    "sos run 20190717_workflow.ipynb fisher \\\n",
    "    --name fisher_0525 --cwd output \\\n",
    "    --genotype-file output/deletion.X_b30.simulation_0525.samples.X.gz \\\n",
    "    --phenotype-file output/deletion.X_b30.simulation_0525.samples.y.gz\n",
    "```\n",
    "\n",
    "### MCMC analysis\n",
    "```\n",
    "sos run 20190717_workflow.ipynb mcmc \\\n",
    "    --name mcmc_0525 --cwd output \\\n",
    "    --genotype-file output/deletion.X_b30.simulation_0525.samples.X.gz \\\n",
    "    --phenotype-file output/deletion.X_b30.simulation_0525.samples.y.gz\n",
    "```\n",
    "\n",
    "### SuSiE analysis\n",
    "```\n",
    "sos run 20190717_workflow.ipynb susie \\\n",
    "    --name susie_0525 --cwd output \\\n",
    "    --genotype-file output/deletion.X_b30.simulation_0525.samples.X.gz \\\n",
    "    --phenotype-file output/deletion.X_b30.simulation_0525.samples.y.gz\n",
    "```\n",
    "\n",
    "### Single Effect Regression analysis\n",
    "```\n",
    "sos run 20190717_workflow.ipynb sier \\\n",
    "    --name sier_0525 --cwd output \\\n",
    "    --genotype-file output/deletion.X_b30.simulation_0525.samples.X.gz \\\n",
    "    --phenotype-file output/deletion.X_b30.simulation_0525.samples.y.gz\n",
    "```\n",
    "\n",
    "### varbvs analysis\n",
    "```\n",
    "sos run 20190717_workflow.ipynb varbvs \\\n",
    "    --name varbvs_0525 --cwd output \\\n",
    "    --genotype-file output/deletion.X_b30.simulation_0525.samples.X.gz \\\n",
    "    --phenotype-file output/deletion.X_b30.simulation_0525.samples.y.gz\n",
    "```\n",
    "\n",
    "### Hybrid approach\n",
    "```\n",
    "sos run 20190717_workflow.ipynb hybrid \\\n",
    "    --name hybrid_0525 --cwd output \\\n",
    "    --genotype-file output/deletion.X_b30.simulation_0525.samples.X.gz \\\n",
    "    --phenotype-file output/deletion.X_b30.simulation_0525.samples.y.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Global configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path(\"output\")\n",
    "parameter: genotype_file = path(\"data/deletion.X.gz\")\n",
    "# phenotype\n",
    "parameter: phenotype_file = path(\"data/deletion.y.gz\")\n",
    "parameter: name = \"\"\n",
    "parameter: blocks_file = path()\n",
    "# MCMC number of iterations\n",
    "parameter: iteration = 5000\n",
    "# MCMC ...\n",
    "parameter: tune_prop = 0.25\n",
    "# MCMC ...\n",
    "parameter: target_accept = 0.98\n",
    "# MCMC ...\n",
    "parameter: n_chain = 10\n",
    "# MCMC ...\n",
    "# For some reason on RCC cluster more than 1 cores will not work (stuck)\n",
    "parameter: n_core = 1\n",
    "# MCMC ...\n",
    "parameter: reparameterize = False\n",
    "## alpha = log(p/1-p) is uniform\n",
    "## lower bound: p = prevalence, when prevalence = 0.05, alpha = -2.94\n",
    "## upper bound: p = case proportion, when case = control, alpha = 0\n",
    "# MCMC ...\n",
    "parameter: prevalence = 0.05\n",
    "# MCMC ...\n",
    "parameter: mcmc_seed = 999\n",
    "# Hyper-parameters for MCMC and for Single Effect Regression\n",
    "parameter: hyperparam_file = path()\n",
    "# SuSiE number of effects\n",
    "parameter: L = 10\n",
    "# Whole genome PIPs obtained by varbvs using `varbvs_wg` pipeline, used for hybrid pipeline\n",
    "parameter: varbvs_wg_pip = path()\n",
    "# cluster job configurations\n",
    "parameter: mcmc_walltime = \"2.5h\"\n",
    "parameter: sier_walltime = \"2h\"\n",
    "parameter: job_size = 80\n",
    "\n",
    "fail_if(len(name) == 0, msg = 'Please specify a name for this analysis using ``--name`` option. This will be used in all relevant output files.')\n",
    "\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Partition genome to blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[genome_partition]\n",
    "# For simulation: get real deletion/duplication CNV data and its block\n",
    "# n_gene_in_block: get_hist: 1, simulate: 20~50, analyze: 1\n",
    "parameter: input_file = str\n",
    "# output contain 3 files: 1) input data removing columns with all zeros, 2) file containing block start and end matching index in 1), 3) block start and end without reindex\n",
    "parameter: output_files = list\n",
    "# minimum number of genes in a block for copy model\n",
    "# set it to 1 to use \"natural blocks\" from input data\n",
    "parameter: n_gene_in_block = int\n",
    "## col_index=None: no row names, col_index=0: use first column as row names\n",
    "parameter: col_index = None\n",
    "input: input_file\n",
    "output: output_files\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '30m', mem = '32G', cores = 1, tags = f'{step_name}_{_output[0]:bn}'\n",
    "python: expand = '${ }', stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    import pandas as pd\n",
    "    from operator import itemgetter\n",
    "    from itertools import *\n",
    "    from collections import OrderedDict\n",
    "    ## header = 0: input with header\n",
    "    data = pd.read_csv(${_input:r}, compression = \"gzip\", sep = \"\\t\", header = 0, index_col = ${col_index})\n",
    "    genes_dict = OrderedDict([(x, y) for x,y in zip([i for i in range(data.shape[1])], list(data.columns))])\n",
    "    data.columns = [i for i in range(data.shape[1])]\n",
    "    data_clean = data.loc[:, (data != 0).any(axis = 0)]\n",
    "    indices = list(data_clean.columns)\n",
    "    bound = list()\n",
    "    i = 0; j = 1; n_0 = len(indices)\n",
    "    while (j < n_0):\n",
    "        if indices[j] - indices[i] >= ${n_gene_in_block} and indices[j] - indices[j-1] > 1:\n",
    "            bound.append([indices[i], indices[j-1]])\n",
    "            i = j\n",
    "        j += 1\n",
    "    bound.append([indices[i], indices[j-1]])\n",
    "    bound = [item for item in bound if item[1] != 0]\n",
    "    if bound[-1] == bound[-2]:\n",
    "        bound = bound[:-1]\n",
    "    ## original index consistent with actual number of genes\n",
    "    pd.DataFrame(bound).to_csv(${_output[2]:r}, sep = \"\\t\", header = False, index = False)\n",
    "    span = [item[1] - item[0] for item in bound]\n",
    "    bound2 = list()\n",
    "    start = 0\n",
    "    for i in span:\n",
    "        end = start + i\n",
    "        start = end + 1\n",
    "        bound2.extend([end, start])\n",
    "    bound2 = [0] + bound2[:-1]\n",
    "    bound3 = [bound2[x:x+2] for x in range(0, len(bound2), 2)]\n",
    "    ## bound3: index start from 0\n",
    "    pd.DataFrame(bound3).to_csv(${_output[1]:r}, sep = \"\\t\", header = False, index = False)\n",
    "    data_clean.columns = [genes_dict[key] for key in list(data_clean.columns)]\n",
    "    data_clean.to_csv(${_output[0]:r}, compression = \"gzip\", sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The output of this step has 3 files:\n",
    "- \"Cleaned\" genotype matrix without column names from boundary: (column names are index from start to end in each boundary and are dropped when the data is saved).\n",
    "\n",
    "        14      15      16      17      18      19      20      21      22      23      93      94    95       96      97\n",
    "         0       1       1       0       1       1       1       1       0       0       0       0     1        1       1\n",
    "         \n",
    "- boundaries corresponding to cleaned data above\n",
    "\n",
    "- original boundaries\n",
    "\n",
    "        block_start     block_end\n",
    "        14      23\n",
    "        93      97\n",
    "        164     177\n",
    "        229     236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[susie_1, varbvs_1, mcmc_1, mcmc_multichain_1, sier_1, hybrid_1, susie_cnv_1, get_hist_1, susie_cnv_hybrid_1]\n",
    "input: genotype_file\n",
    "output: f\"{cwd:a}/{_input:bn}.cleaned.gz\", f\"{cwd:a}/{_input:bn}.block_index.csv\", f\"{cwd:a}/{_input:bn}.block_index_original.csv\"\n",
    "sos_run('genome_partition', input_file = _input, output_files = _output, n_gene_in_block = 1, col_index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Gene level Fisher's exact test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fisher_2]\n",
    "output: f\"{_input[0]:n}.fisher.gz\"\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '30m', mem = '4G', cores = 1, tags = f'{step_name}_{_output:bn}'\n",
    "python: expand = '${ }'\n",
    "    import pandas as pd\n",
    "    ## use stats.fisher_exact instead of \"from fisher import pvalue\", because \"pvalue\" does not generate the constant pvalue, \n",
    "    ## for example, 'pvalue(56,6650,0,6706).two_tail' is not more significant than 'pvalue(24,6682,0,6706).two_tail'\n",
    "    from scipy import stats\n",
    "    data = pd.read_csv(${_input[0]:r}, compression = \"gzip\", sep = \"\\t\", header = 0)\n",
    "    y = pd.read_csv(\"${phenotype_file}\", header = None, names = [\"y\"])\n",
    "    xy = pd.concat([y, data], axis = 1, join = 'inner')\n",
    "    xy1 = xy[xy[\"y\"] == 1]\n",
    "    n1 = xy1.shape[0]\n",
    "    xy0 = xy[xy[\"y\"] == 0]\n",
    "    n0 = xy0.shape[0]\n",
    "    res = list()\n",
    "    for i in list(data.columns):\n",
    "        res.append([i, sum(xy1.loc[:,i]), n1 - sum(xy1.loc[:,i]), sum(xy0.loc[:,i]), n0 - sum(xy0.loc[:,i]), \n",
    "                    stats.fisher_exact([[sum(xy1.loc[:,i]), sum(xy0.loc[:,i])], [n1 - sum(xy1.loc[:,i]), n0 - sum(xy0.loc[:,i])]])[1]])\n",
    "    pd.DataFrame(res).sort_values(by = 5).to_csv(${_output:r}, compression = \"gzip\", sep = \"\\t\", header = [\"gene\", \"d_c\", \"d_nc\", \"nd_c\", \"nd_nc\", \"p\"], index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Obtain independent CNV blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[susie_2, varbvs_2, mcmc_2, mcmc_multichain_2, sier_2, hybrid_2, susie_cnv_2, susie_cnv_hybrid_2]\n",
    "## R: fread(${_input:[0]}, select = ${_blocks.replace('_', ':')})\n",
    "## similar to fine mapping, create 527 folders and save results for each of them\n",
    "if os.path.isfile(f'{blocks_file:a}'):\n",
    "    blocks = [tuple(x.strip().split()) for x in open(f'{blocks_file:a}').readlines() if not x.strip().startswith('#')]\n",
    "else:\n",
    "    blocks = [tuple(x.strip().split()) for x in open(f'{_input[1]:a}').readlines()]\n",
    "input: for_each = ['blocks']\n",
    "output: f\"{_input[0]:nn}_block_{_blocks[0]}_{_blocks[1]}/block_{_blocks[0]}_{_blocks[1]}.gz\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '2m', mem = '16G', cores = 1, tags = f'{step_name}_{_output:bn}'\n",
    "python: expand = '${ }'\n",
    "    import pandas as pd, numpy as np\n",
    "    import os\n",
    "    start, end = map(int, ${_blocks})\n",
    "    data = pd.read_csv(${_input[0]:r}, compression = \"gzip\", sep = \"\\t\", header = 0)\n",
    "    data.iloc[:, start:end+1].to_csv(${_output:r}, compression = \"gzip\", sep = \"\\t\", header = True, index = False)\n",
    "    if os.path.isfile(${varbvs_wg_pip:r}):\n",
    "        varbvs_wg_pips = [x for x in open(${varbvs_wg_pip:r}).readlines()][start:(end+1)]\n",
    "        print(''.join(varbvs_wg_pips), file = open(\"${_output:n}.varbvs_pip\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Method `susie-cnv-hybrid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = x, srcfile = src): <text>:1:1: unexpected '['\n1: [\n    ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = x, srcfile = src): <text>:1:1: unexpected '['\n1: [\n    ^\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "[susie_cnv_hybrid_3]\n",
    "depends: hyperparam_file, R_library('susieR'), R_library('logistf')\n",
    "output: f\"{_input:n}.{name}.pip\"\n",
    " \n",
    "fail_if(not os.path.isfile(f\"{_input:n}.varbvs_pip\"), msg = f\"Cannot find ``{_input:n}.varbvs_pip`` for use with hybrid pipeline!\")\n",
    "expected_effects = np.sum(np.loadtxt(f\"{_input:n}.varbvs_pip\", dtype=np.float64, usecols = 1))\n",
    "print(expected_effects, file=open(f\"{_output:n}.n_effects\", 'w'))\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = sier_walltime, mem = '8G', cores = 1, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    library('susieR')\n",
    "    library('logistf')\n",
    "    # logistic_regression <- function(X, y){\n",
    "    #   output <- matrix(0,ncol(X),2)\n",
    "    #   for (j in 1:ncol(X)){\n",
    "    #     output[j,] <- as.vector(summary(glm(y ~ X[,j], family = \"binomial\"))$coef[2,1:2])\n",
    "    #   }\n",
    "    #   return(list(betahat = output[,1], sebetahat = output[,2]))\n",
    "    # }\n",
    "    firth.logit.reg <- function(X,y){\n",
    "        outputs <- matrix(0,ncol(X),3)\n",
    "        for (j in 1:ncol(X)){\n",
    "            fit <- logistf(y ~ X[,j])\n",
    "            outputs[j,1] <- fit$coefficients[2]\n",
    "            outputs[j,2] <- sqrt(diag(vcov(fit)))[2]\n",
    "            outputs[j,3] <- fit$prob[2]\n",
    "        }\n",
    "        return(list(betahat = outputs[,1], sebetahat = outputs[,2], p = outputs[,3]))\n",
    "    }\n",
    "    ###\n",
    "    ###\n",
    "    X <- as.matrix(read.table(gzfile(${_input:r}), header = TRUE))\n",
    "    y <- as.matrix(read.table(\"${phenotype_file}\"))\n",
    "    priors <- read.table(\"${hyperparam_file}\")\n",
    "    \n",
    "    sumstats    <- firth.logit.reg(X,y)\n",
    "    R           <- cor(X)\n",
    "    pi          <- priors[1,1]\n",
    "    s0          <- priors[3,1]\n",
    "    null_weight <- (1 - pi)^ncol(X)\n",
    "    phi         <- sum(y) / length(y)\n",
    "    var.y       <- 1 / (phi * (1 - phi))\n",
    "    n           <- nrow(X)\n",
    "\n",
    "    if (${expected_effects} >= 1){\n",
    "        res <- susie_rss(R = R, n = n, L = ceiling(${expected_effects}) + 1,\n",
    "                         bhat = sumstats$betahat,\n",
    "                         shat = sumstats$sebetahat,\n",
    "                         var_y = var.y,\n",
    "                         scaled_prior_variance = s0^2 / var.y,\n",
    "                         estimate_prior_variance = FALSE,\n",
    "                         null_weight = null_weight,\n",
    "                         standardize = FALSE,\n",
    "                         check_prior = FALSE)\n",
    "    } else {\n",
    "        suppressWarnings({\n",
    "        res <- susie_rss(R = R, n = n, L = 1,\n",
    "                         bhat = sumstats$betahat,\n",
    "                         shat = sumstats$sebetahat,\n",
    "                         var_y = var.y,\n",
    "                         scaled_prior_variance = s0^2 / var.y,\n",
    "                         estimate_prior_variance = FALSE,\n",
    "                         null_weight = null_weight,\n",
    "                         max_iter = 1,   \n",
    "                         standardize = FALSE,\n",
    "                         check_prior = FALSE)\n",
    "        })\n",
    "    }\n",
    "\n",
    "    names(res$pip) = colnames(X)\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Method `hybrid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[hybrid_3]\n",
    "output: f\"{_input:n}.{name}.pip\"\n",
    "fail_if(not os.path.isfile(f\"{_input:n}.varbvs_pip\"), msg = f\"Cannot find ``{_input:n}.varbvs_pip`` for use with hybrid pipeline!\")\n",
    "expected_effects = np.sum(np.loadtxt(f\"{_input:n}.varbvs_pip\", dtype=np.float64, usecols = 1))\n",
    "print(expected_effects, file=open(f\"{_output:n}.n_effects\", 'w'))\n",
    "sos_run('mcmc:3' if expected_effects >= 1 else 'sier:3', expected_effects = expected_effects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Method `mcmc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mcmc_multichain_3]\n",
    "depends: hyperparam_file\n",
    "output: f\"{_input:n}.{name}.pip\"\n",
    "## when chain = 3: walltime = 3h, max_walltime = 100, jobsize = 8\n",
    "## when chain = 1: walltime = 20m, job = 5, max_walltime = 36\n",
    "## when chain = 5: walltime = 3h, mem = '12G'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = mcmc_walltime, mem = '12G', cores = n_core, tags = f'{step_name}_{_output:bn}'\n",
    "python: expand = '${ }', env={'THEANO_FLAGS': f\"base_compiledir={_output:d}\"}, stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    import numpy as np, pandas as pd, pymc3 as pm, theano.tensor as tt\n",
    "    import os\n",
    "    from scipy.special import expit\n",
    "    priors = np.loadtxt(\"${hyperparam_file}\")\n",
    "    pi0, mu0, s0 = priors[0], priors[1], priors[2]\n",
    "    X_complete = pd.read_csv(${_input:r}, compression = \"gzip\", sep = \"\\t\", header = 0, dtype = float)\n",
    "    # remove duplicated columns in X but still keep track of the number of occurance and index removed \n",
    "    # in order to reconstruct results later\n",
    "    X, index_reconstruct, dup_counts = np.unique(X_complete.to_numpy(), axis=1, return_inverse=True, return_counts=True)\n",
    "    y = np.loadtxt(\"${phenotype_file}\", dtype = int)\n",
    "    case_prop = sum(y) / y.shape[0]\n",
    "    invlogit = lambda x: 1/(1 + tt.exp(-x))\n",
    "    upper = np.log(case_prop / (1-case_prop))\n",
    "    lower = np.log(${prevalence} / (1-${prevalence}))\n",
    "    model = pm.Model()\n",
    "    with model:\n",
    "        xi = pm.Bernoulli('xi', pi0, shape = X.shape[1]) #inclusion probability for each variable\n",
    "        if ${reparameterize}:\n",
    "            beta_offset = pm.Normal('beta_offset', mu = 0, sd = 1, shape = X.shape[1])\n",
    "            alpha_offset = pm.distributions.continuous.Uniform(\"alpha_offset\", lower = -1, upper = 1)\n",
    "            beta = pm.Deterministic(\"beta\", mu0 + beta_offset * s0) #Prior for the non-zero coefficients\n",
    "            alpha = pm.Deterministic(\"alpha\", lower + (alpha_offset+1)/2*(upper - lower))\n",
    "        else:\n",
    "            beta = pm.Normal('beta', mu = mu0, sd = s0, shape = X.shape[1])\n",
    "            alpha = pm.distributions.continuous.Uniform(\"alpha\", lower = lower, upper = upper)\n",
    "        p = pm.math.dot(X, xi * beta) #Deterministic function to map the stochastics to the output\n",
    "        y_obs = pm.Bernoulli('y_obs', invlogit(p + alpha), observed = y) #Data likelihood\n",
    "    with model:\n",
    "        trace = pm.sample(draws = ${iteration}, init='nuts', chains = ${n_chain}, tune = int(${tune_prop} * ${iteration}),\n",
    "                        nuts = {\"target_accept\": ${target_accept}},\n",
    "                        random_seed = ${mcmc_seed}, cores = min(${n_core}, ${n_chain}), progressbar = True)\n",
    "    # FIXME: dump trace to pkl here, if needed\n",
    "    # results\n",
    "    pip = np.apply_along_axis(np.mean, 0, trace['xi'])\n",
    "    beta = np.apply_along_axis(np.mean, 0, np.multiply(trace[\"beta\"], trace[\"xi\"]))\n",
    "    beta_given_inclusion = np.apply_along_axis(np.sum, 0, trace['xi'] * trace['beta']) / np.apply_along_axis(np.sum, 0, trace['xi'])\n",
    "    # reconstruct original results adding back duplicated variables\n",
    "    pip = pip / dup_counts\n",
    "    beta = beta / dup_counts\n",
    "    results = np.vstack((pip, beta, beta_given_inclusion)).T[index_reconstruct,:]\n",
    "    results = pd.DataFrame(results, columns = ['inclusion_probability', 'beta', 'beta_given_inclusion'])\n",
    "    results = results.set_index(X_complete.columns)\n",
    "    results[[\"inclusion_probability\"]].to_csv(${_output:r}, sep = \"\\t\", header = False, index = True)\n",
    "    results.to_csv(\"${_output:n}.pip_beta\", sep = \"\\t\", header = False, index = True)\n",
    "    print(\"intercept: uniform\\nn_chain: ${n_chain}\\nn_iter: ${iteration}\\ntune_prop: ${tune_prop}\\nseed: ${mcmc_seed}\\ntarget_accept: ${target_accept}\", file=open(\"${_output:n}.yml\", 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Multi-chain MCMC is meant to help and diagnose convergence. It is essentially running MCMC at different starting points and with different seed. However due to limitation of pymc3 we cannot save multi-chain samples in reasonable amount of memory. The function to save them to files (`trace` option in `sample` function with different backends) are very buggy as of now, and could be depercated in the future according to developers.\n",
    "\n",
    "Here we implement a version to run multiple single chains and combine the results. They will use different seeds and with `start = \"nuts\"` they should be using different starting points too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mcmc_3]\n",
    "depends: hyperparam_file\n",
    "output: f\"{_input:n}.{name}.pip\"\n",
    "## when chain = 3: walltime = 3h, max_walltime = 100, jobsize = 8\n",
    "## when chain = 1: walltime = 20m, job = 5, max_walltime = 36\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = mcmc_walltime, mem = '8G', cores = 1, tags = f'{step_name}_{_output:bn}'\n",
    "python: expand = '${ }', env={'THEANO_FLAGS': f\"base_compiledir={_output:d}\"}, stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    import numpy as np, pandas as pd, pymc3 as pm, theano.tensor as tt\n",
    "    import os\n",
    "    from scipy.special import expit\n",
    "    priors = np.loadtxt(\"${hyperparam_file}\")\n",
    "    pi0, mu0, s0 = priors[0], priors[1], priors[2]\n",
    "    X_complete = pd.read_csv(${_input:r}, compression = \"gzip\", sep = \"\\t\", header = 0, dtype = float)\n",
    "    # remove duplicated columns in X but still keep track of the number of occurance and index removed \n",
    "    # in order to reconstruct results later\n",
    "    X, index_reconstruct, dup_counts = np.unique(X_complete.to_numpy(), axis=1, return_inverse=True, return_counts=True)\n",
    "    y = np.loadtxt(\"${phenotype_file}\", dtype = int)\n",
    "    case_prop = sum(y) / y.shape[0]\n",
    "    invlogit = lambda x: 1/(1 + tt.exp(-x))\n",
    "    upper = np.log(case_prop / (1-case_prop))\n",
    "    lower = np.log(${prevalence} / (1-${prevalence}))\n",
    "    model = pm.Model()\n",
    "    with model:\n",
    "        xi = pm.Bernoulli('xi', pi0, shape = X.shape[1]) #inclusion probability for each variable\n",
    "        if ${reparameterize}:\n",
    "            beta_offset = pm.Normal('beta_offset', mu = 0, sd = 1, shape = X.shape[1])\n",
    "            alpha_offset = pm.distributions.continuous.Uniform(\"alpha_offset\", lower = -1, upper = 1)\n",
    "            beta = pm.Deterministic(\"beta\", mu0 + beta_offset * s0) #Prior for the non-zero coefficients\n",
    "            alpha = pm.Deterministic(\"alpha\", lower + (alpha_offset+1)/2*(upper - lower))\n",
    "        else:\n",
    "            beta = pm.Normal('beta', mu = mu0, sd = s0, shape = X.shape[1])\n",
    "            alpha = pm.distributions.continuous.Uniform(\"alpha\", lower = lower, upper = upper)\n",
    "        p = pm.math.dot(X, xi * beta) #Deterministic function to map the stochastics to the output\n",
    "        y_obs = pm.Bernoulli('y_obs', invlogit(p + alpha), observed = y) #Data likelihood\n",
    "    # Fit model multiple times\n",
    "    results = []\n",
    "    for i in range(${n_chain}):\n",
    "        with model:\n",
    "            trace = pm.sample(draws = ${iteration}, init = 'nuts', chains = 1, tune = int(${tune_prop} * ${iteration}),\n",
    "                              nuts = {\"target_accept\": ${target_accept}},\n",
    "                              random_seed = ${mcmc_seed} + i, cores = 1, progressbar = True)\n",
    "        # FIXME: dump trace to pkl here, if needed\n",
    "        # results\n",
    "        pip = np.apply_along_axis(np.mean, 0, trace['xi'])\n",
    "        beta = np.apply_along_axis(np.mean, 0, np.multiply(trace[\"beta\"], trace[\"xi\"]))\n",
    "        beta_given_inclusion = np.apply_along_axis(np.sum, 0, trace['xi'] * trace['beta']) / np.apply_along_axis(np.sum, 0, trace['xi'])\n",
    "        # reconstruct original results adding back duplicated variables\n",
    "        pip = pip / dup_counts\n",
    "        beta = beta / dup_counts\n",
    "        result = np.vstack((pip, beta, beta_given_inclusion)).T[index_reconstruct,:]\n",
    "        results.append(pd.DataFrame(result, columns = ['inclusion_probability', 'beta', 'beta_given_inclusion']))\n",
    "    # merge results\n",
    "    results = sum(results)/len(results)\n",
    "    results = results.set_index(X_complete.columns)\n",
    "    results[[\"inclusion_probability\"]].to_csv(${_output:r}, sep = \"\\t\", header = False, index = True)\n",
    "    results.to_csv(\"${_output:n}.pip_beta\", sep = \"\\t\", header = False, index = True)\n",
    "    print(\"intercept: uniform\\nn_chain: ${n_chain}\\nn_iter: ${iteration}\\ntune_prop: ${tune_prop}\\nseed: ${mcmc_seed}\\ntarget_accept: ${target_accept}\", file=open(\"${_output:n}.yml\", 'w'))\n",
    "\n",
    "bash: expand = True\n",
    "    rm -rf {_output:d}/compiledir_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Method `sier` (single effect regression)\n",
    "\n",
    "Set `--expected-effects` to `1` to use the original single effect model. Otherwise based on input `--expected-effects` and availablility of `varbvs` based regional PIP information, the output PIP will be weighted by expected number of effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[sier_3]\n",
    "depends: R_library(\"data.table\"), hyperparam_file\n",
    "parameter: expected_effects=-9\n",
    "if expected_effects < 0:\n",
    "    expected_effects = min(1, np.sum(np.loadtxt(f\"{_input:n}.varbvs_pip\", dtype=np.float64, usecols = 1))) if os.path.isfile(f\"{_input:n}.varbvs_pip\") else 1\n",
    "output: f\"{_input:n}.{name}.pip\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = sier_walltime, mem = '8G', cores = 1, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    # Compute useful posterior statistics in a simple \"single effect\"\n",
    "    # Bayesian logistic regression, in which y ~ sigmoid(b0 + x*b) and b ~\n",
    "    # N(mu0,s0), where b0 is the intercept, b is the regression\n",
    "    # coefficient, and mu0, s0 are the the prior mean and standard\n",
    "    # deviation of b. (The intercept is assigned a \"flat\" prior.)\n",
    "    #\n",
    "    # Input X is the n x p \"data matrix\", where n is the number of samples\n",
    "    # and p is the number of candidate predictors, and input y is the\n",
    "    # vector of n observed outcomes.\n",
    "    #\n",
    "    # Input argument p1 specifies the prior inclusion probabilities; it\n",
    "    # should be a vector of length p in which p1[i]/sum(p1) gives the\n",
    "    # prior probability that the jth candidate predictor has a nonzero\n",
    "    # effect on the outcome, Y.\n",
    "    #\n",
    "    # Note that these computations could probably be made faster and more\n",
    "    # accurate using a fast 2-d numerical integration (quadrature) method.\n",
    "    bayes.logistic <- function (X, y, p0, mu0, s0) {\n",
    "\n",
    "      # These two variables define the 2-d grid used to compute the Monte\n",
    "      # Carlo (importance sampling) estimates.\n",
    "      b0  <- seq(-10,10,0.1) \n",
    "      b   <- seq(-10,10,0.1)\n",
    "\n",
    "      # Create the 2-d grid.\n",
    "      out <- expand.grid(b0 = b0,b = b)\n",
    "      b0  <- out$b0\n",
    "      b   <- out$b\n",
    "      rm(out)\n",
    "\n",
    "      # Get the number of candidate predictors (p) and importance weights (n).\n",
    "      p <- ncol(X)\n",
    "      n <- length(b)\n",
    "\n",
    "      # Initialize storage for the marginal log-likelihoods (logZ) and the\n",
    "      # posterior means (mu1) and standard deviations (s1) of the\n",
    "      # coefficients.\n",
    "      logZ <- rep(0,p)\n",
    "      mu1  <- rep(0,p)\n",
    "      s1   <- rep(0,p)\n",
    "\n",
    "      # Repeat for each candidate predictor.\n",
    "      for (j in 1:p)  {\n",
    "\n",
    "        # Compute the log-importance weights, ignoring constant terms. The\n",
    "        # important weight is a product of the (logistic) likelihood and\n",
    "        # the (normal) prior. This is the step that requires the most\n",
    "        # effort.\n",
    "        logw <- rep(0,n)\n",
    "        x    <- X[,j]\n",
    "        for (i in 1:n) {\n",
    "          u       <- b0[i] + x*b[i]\n",
    "          logw[i] <- sum((y - 1)*u + logsigmoid(u)) - ((b[i] - mu0)/s0)^2/2\n",
    "        }\n",
    "\n",
    "        # Compute the importance sampling estimate of the marginal\n",
    "        # log-likelihood (up to a constant of proportionality).\n",
    "        u       <- max(logw)\n",
    "        logZ[j] <- log(mean(exp(logw - u))) + u\n",
    "\n",
    "        # Compute the normalized importance weights.\n",
    "        w <- softmax(logw)\n",
    "\n",
    "        # Compute the mean and standard deviation of the coefficient.\n",
    "        mu1[j] <- sum(w*b)\n",
    "        s1[j]  <- sqrt(sum(w*(b - mu1[j])^2))\n",
    "      }\n",
    "\n",
    "      # Compute the posterior inclusion probabilities.\n",
    "      p1 <- softmax(logZ + log(p0))\n",
    "\n",
    "      # Output the data frame containing the computed posterior\n",
    "      # statistics.\n",
    "      return(data.frame(p1 = p1,mu1 = mu1,s1 = s1))\n",
    "    }\n",
    "    # sigmoid(x) returns the sigmoid of the elements of x. The sigmoid\n",
    "    # function is also known as the logistic link function. It is the\n",
    "    # inverse of logit(x).\n",
    "    sigmoid <- function (x) {\n",
    "      y   <- x\n",
    "      y[] <- 0\n",
    "      y[x > -500] <- 1/(1 + exp(-x))\n",
    "      return(y)\n",
    "    }\n",
    "\n",
    "    # logpexp(x) returns log(1 + exp(x)). The computation is performed in a\n",
    "    # numerically stable manner. For large entries of x, log(1 + exp(x)) is\n",
    "    # effectively the same as x.\n",
    "    logpexp <- function (x) {\n",
    "      y    <- x\n",
    "      i    <- which(x < 16)\n",
    "      y[i] <- log(1 + exp(x[i]))\n",
    "      return(y)\n",
    "    }\n",
    "\n",
    "    # Use this instead of log(sigmoid(x)) to avoid loss of numerical precision.\n",
    "    logsigmoid <- function (x)\n",
    "      -logpexp(-x)\n",
    "\n",
    "    # Compute the softmax of vector x in a more numerically prudent manner\n",
    "    # that avoids overflow or underflow.\n",
    "    softmax <- function (x) {\n",
    "      y <- exp(x - max(x))\n",
    "      return(y/sum(y))\n",
    "    }\n",
    "    ###\n",
    "    ###\n",
    "    ###\n",
    "    X <- as.matrix(read.table(gzfile(${_input:r}), header = TRUE))\n",
    "    X <- scale(X, center = TRUE, scale = FALSE)\n",
    "    y <- as.matrix(read.table(\"${phenotype_file}\"))\n",
    "    p <- dim(X)[2]\n",
    "    priors <- read.table(\"${hyperparam_file}\")\n",
    "    p0 <- rep(priors[1,1], 1, p)\n",
    "    print (\"Fitting Bayesian logistic regression model ...\")\n",
    "    out <- bayes.logistic(X, y, p0, priors[2,1], priors[3,1])\n",
    "    print (\"Model fitting completed!\")\n",
    "    pip <- out$p1 * ${expected_effects}\n",
    "    names(pip) <- colnames(X)\n",
    "    write.table(t(t(pip)), ${_output:r}, sep='\\t', col.names=FALSE, quote=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Method `SuSiE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[susie_3]\n",
    "depends: R_library('susieR'), hyperparam_file\n",
    "parameter: estimate_prior_method = \"simple\"\n",
    "parameter: check_null_threshold = 0.1\n",
    "output: f\"{_input:n}.{name}.rds\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '5m', mem = '6G', cores = 1, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    library(susieR)\n",
    "    X <- as.matrix(read.table(gzfile(${_input:r}), header = TRUE))\n",
    "    y <- as.matrix(read.table(\"${phenotype_file}\"))\n",
    "    storage.mode(X) = 'double'\n",
    "    storage.mode(y) = 'double'\n",
    "    \n",
    "    # load priors from global estimates and fix it\n",
    "    priors      <- read.table(\"${hyperparam_file}\")\n",
    "    s0          <- priors[3,1]\n",
    "    pi          <- priors[1,1]\n",
    "    null_weight <- (1 - pi)^ncol(X)\n",
    "    \n",
    "    # Gao and Min's first version of SuSiE\n",
    "    #res = susie(X, y, L = ${L}, \n",
    "    #            scaled_prior_variance = pve, \n",
    "    #            estimate_residual_variance = TRUE,\n",
    "    #            estimate_prior_variance = TRUE,\n",
    "    #            check_null_threshold = ${check_null_threshold},\n",
    "    #            null_weight = null_weight)\n",
    "    \n",
    "    # Newly implemented SuSiE\n",
    "    #res <- susie(X, y, L = ${L},\n",
    "    #             scaled_prior_variance = (s0^2) / var(y),\n",
    "    #             null_weight = null_weight,\n",
    "    #             estimate_prior_variance = FALSE,\n",
    "    #             standardize = FALSE)\n",
    "    \n",
    "    # Run default susie\n",
    "    res <- susie(X, y, L=${L}, null_weight = null_weight)\n",
    "    \n",
    "    \n",
    "    names(res$pip) = colnames(X)\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mcmc_4, mcmc_multichain_4, sier_4, hybrid_4]\n",
    "input: group_by = \"all\"\n",
    "output: f\"{cwd:a}/{phenotype_file:bn}.{name}_pip.gz\"\n",
    "bash: expand = '${ }'\n",
    "    cat ${_input} | gzip --best > ${_output:r}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Method `susie_cnv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[susie_cnv_3]\n",
    "depends: R_library('susieR')\n",
    "parameter: check_null_threshold = 0.1\n",
    "output: f\"{_input:n}.{name}.rds\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '5m', mem = '6G', cores = 1, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    library('susieR')\n",
    "    library('logistf')\n",
    "    \n",
    "    # Summary statistics by logistic regression\n",
    "    # logistic_regression <- function(X, y){\n",
    "    #   output = matrix(0,ncol(X),2)\n",
    "    #   for (j in 1:ncol(X)){\n",
    "    #     output[j,] <- as.vector(summary(glm(y ~ X[,j], family = \"binomial\"))$coef[2,1:2])\n",
    "    #   }\n",
    "    #   return(list(betahat = output[,1], sebetahat = output[,2]))\n",
    "    # }\n",
    "    firth.logit.reg <- function(X,y){\n",
    "        outputs <- matrix(0,ncol(X),3)\n",
    "        for (j in 1:ncol(X)){\n",
    "            fit <- logistf(y ~ X[,j])\n",
    "            outputs[j,1] <- fit$coefficients[2]\n",
    "            outputs[j,2] <- sqrt(diag(vcov(fit)))[2]\n",
    "            outputs[j,3] <- fit$prob[2]\n",
    "        }\n",
    "        return(list(betahat = outputs[,1], sebetahat = outputs[,2], p = outputs[,3]))\n",
    "    }\n",
    "    ###\n",
    "    ###\n",
    "    ###\n",
    "    X <- as.matrix(read.table(gzfile(${_input:r}), header = TRUE))\n",
    "    y <- as.matrix(read.table(\"${phenotype_file}\"))\n",
    "    priors <- read.table(\"${hyperparam_file}\")\n",
    "    #storage.mode(X) <- 'double'\n",
    "    #storage.mode(y) <- 'double'\n",
    "    \n",
    "    sumstats    <- firth.logit.reg(X,y)\n",
    "    R           <- cor(X)\n",
    "    s0          <- priors[3,1]\n",
    "    pi          <- priors[1,1]\n",
    "    null_weight <- (1 - pi)^ncol(X)\n",
    "    phi         <- sum(y) / length(y)\n",
    "    var.y       <- 1 / (phi * (1 - phi))\n",
    "    \n",
    "    # Version of newly implemented susie-rss\n",
    "    res <- susie_rss(R = R, n = nrow(X), L = 10,\n",
    "                     bhat = sumstats$betahat, \n",
    "                     shat = sumstats$sebetahat,\n",
    "                     var_y = var.y,\n",
    "                     scaled_prior_variance = (s0^2) / var.y,\n",
    "                     estimate_prior_variance = FALSE,\n",
    "                     null_weight = null_weight,\n",
    "                     standardize = FALSE,\n",
    "                     check_prior = FALSE)\n",
    "    \n",
    "    #res <- susie_rss(R = R, n = nrow(X),\n",
    "    #                 bhat = sumstats$betahat, \n",
    "    #                 shat = sumstats$sebetahat, \n",
    "    #                 var_y = var(y),\n",
    "    #                 L = 10,\n",
    "    #                 scaled_prior_variance = pve*sqrt(nrow(X)),\n",
    "    #                 estimate_residual_variance = TRUE,\n",
    "    #                 estimate_prior_variance = TRUE,\n",
    "    #                 check_null_threshold = ${check_null_threshold},\n",
    "    #                 null_weight = null_weight)\n",
    "    \n",
    "    names(res$pip) = colnames(X)\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[susie_4, varbvs_4, logisticsusie_4, susie_cnv_4, susie_cnv_hybrid_4]\n",
    "input: group_by = \"all\"\n",
    "output:  f\"{cwd:a}/{phenotype_file:bn}.{name}_pip.gz\"\n",
    "R: expand = '${ }', stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    files = c(${_input:r,})\n",
    "    pips = c()\n",
    "    for (i in 1:length(files)){pips = c(pips, readRDS(files[i])$pip)}\n",
    "    write.table(t(t(pips)), gzfile(${_output:r}), sep='\\t', col.names=FALSE, quote=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Method `logisticsusie`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[logisticsusie_3]\n",
    "depends: R_library('logisticsusie'), hyperparam_file\n",
    "parameter: check_null_threshold = 0.1\n",
    "output: f\"{_input:n}.{name}.rds\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = sier_walltime, mem = '8G', cores = 1, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    library('logisticsusie')\n",
    "    X <- as.matrix(read.table(gzfile(${_input:r}), header = TRUE))\n",
    "    y <- as.matrix(read.table(\"${phenotype_file}\"))[,1]\n",
    "    storage.mode(X) = 'double'\n",
    "    storage.mode(y) = 'double'\n",
    "    priors = read.table(\"${hyperparam_file}\")\n",
    "    res = binsusie(X, y, N=1, prior_mean = priors[2,1], prior_variance = priors[3,1]^2, check_null_threshold = ${check_null_threshold})\n",
    "    names(res$pip) = colnames(X)\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Method `varbvs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[varbvs_3]\n",
    "depends: R_library(\"varbvs\")\n",
    "output: f\"{_input:n}.{name}.rds\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '5m', mem = '6G', cores = 1, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    X <- as.matrix(read.table(gzfile(${_input:r}), header = TRUE))\n",
    "    y <- as.matrix(read.table(\"${phenotype_file}\"))\n",
    "    storage.mode(X) = 'double'\n",
    "    storage.mode(y) = 'double'\n",
    "    # logodds <- seq(-log10(ncol(X)), 0, length.out = 20)\n",
    "    fit <- varbvs::varbvs(X, NULL, y, family = \"binomial\", verbose = FALSE)\n",
    "    names(fit$pip) = colnames(X)\n",
    "    saveRDS(fit, ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Whole genome analysis using `varbvs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[varbvs_wg]\n",
    "depends: R_library(\"varbvs\")\n",
    "parameter: maximum_prior_inclusion = 0.0\n",
    "parameter: Rseed = 999\n",
    "input: genotype_file, phenotype_file\n",
    "output: f\"{cwd:a}/{_input[0]:bnn}.{name}.rds\", f\"{cwd:a}/{_input[0]:bnn}.{name}.pip\", f\"{cwd:a}/{_input[0]:bnn}.{name}.hyperparams\"\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '6h', mem = '16G', cores = 1, tags = f'{step_name}_{_output:bn}'\n",
    "python: expand = \"${ }\"\n",
    "    import pandas as pd\n",
    "    data = pd.read_csv(${_input[0]:r}, compression = \"gzip\", sep = \"\\t\", header = 0)\n",
    "    data_clean = data.loc[:, (data != 0).any(axis = 0)]\n",
    "    data_clean.to_csv(\"${_input[0]:n}.cleaned.gz\", compression = \"gzip\", sep = \"\\t\", header = True, index = False)\n",
    "\n",
    "R: expand = '${ }'\n",
    "    set.seed(${Rseed})\n",
    "    X <- as.matrix(read.table(gzfile(\"${_input[0]:n}.cleaned.gz\"), header = TRUE))\n",
    "    y <- as.matrix(read.table(${_input[1]:r}))\n",
    "    storage.mode(X) = 'double'\n",
    "    storage.mode(y) = 'double'\n",
    "    if (${maximum_prior_inclusion}>0) {\n",
    "        n_grid = 20\n",
    "        q = (1:ceiling(n_grid*${maximum_prior_inclusion})) / n_grid\n",
    "    }\n",
    "    \n",
    "    # Task 1: Fit logistic regression BVSR to obtain hyperparameter estimates for spike slab model, and PIP\n",
    "    fit = varbvs::varbvs(X, NULL, y, family = \"binomial\", update.sa = TRUE, update.b0 = TRUE, ${'logodds = log10(q/(1-q)),' if maximum_prior_inclusion > 0 else ''} verbose = FALSE)\n",
    "    names(fit$pip) <- colnames(X)\n",
    "    write.table(t(t(fit$pip)), ${_output[1]:r}, sep='\\t', col.names=FALSE, quote=FALSE)\n",
    "    sigmoid = function(x) 1 / (1 + exp(-x))\n",
    "    pi1 = sigmoid(log(10) * sum(fit$logodds * fit$w))\n",
    "    mu0 = sum(fit$b0 * fit$w)\n",
    "    s0 = sqrt(sum(fit$sa * fit$w))\n",
    "    \n",
    "    # Task 2: Fit a linear regression BVSR to obtain estimate for prior variance (\"PVE\") for downstream SuSiE analysis\n",
    "    fit_lm = varbvs::varbvs(X, NULL, y, family = \"gaussian\", update.sa = TRUE, update.b0 = TRUE, ${'logodds = log10(q/(1-q)),' if maximum_prior_inclusion > 0 else ''} verbose = FALSE)\n",
    "    sigmoid = function(x) 1 / (1 + exp(-x))\n",
    "    # Get posterior PVE and adjust that by number of causal gene\n",
    "    # Here I use pi1 estimated from logistic BVSR as prior inclusion probability.\n",
    "    # The same pi1 will be used to set prior null weight in SuSiE\n",
    "    total_pve = mean(fit_lm$model.pve)\n",
    "    ## posterior pve\n",
    "    pve = total_pve / (ncol(X) * pi1)\n",
    "    # Here I use prior PVE which is sa * sigma\n",
    "    # pve = sum(fit_lm$sa * fit_lm$w) * sum(fit_lm$sigma * fit_lm$w)\n",
    "    write.table(c(pi1,mu0,s0,pve), file = ${_output[2]:r}, sep = \"\\n\", row.names = FALSE, col.names = FALSE)\n",
    "    saveRDS(list(logit=fit, lm=fit_lm), ${_output[0]:r})\n",
    "\n",
    "bash: expand = True\n",
    "    rm -f {_input[0]:n}.cleaned.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Histogram for gene count per CNV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[get_hist_2]\n",
    "output: f\"{_input[0]:n}.{name}.pdf\"\n",
    "python: expand = '${ }'\n",
    "    import pandas as pd, matplotlib.pyplot as plt\n",
    "    blocks = pd.read_csv(${_input[1]:r}, sep = \"\\t\", header = None, names = [\"start\", \"end\"])\n",
    "    spans = [j-i+1 for i,j in zip(blocks[\"start\"], blocks[\"end\"])]\n",
    "    counts = {i: spans.count(i) for i in set(spans) if i != 0}\n",
    "    fig, ax = plt.subplots(figsize = (8,6))\n",
    "    plt.bar(list(counts.keys()), list(counts.values()), width = 0.8)\n",
    "    ax.set_title(\"Histogram of number of genes in blocks\")\n",
    "    plt.savefig(${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Simulate CNV blocks\n",
    "For numerical studies in the manuscript. Input has to use all genotypes including zeros, because in copy model based simulation we have to use actual genome positions, and genotype data without removing any genes is a proxy for genotype positions. Here it is not exactly copy model in the context of SNPs where block size can be based on knowledge of human LD block size -- at least 30 genes per CNV block is based on empirical experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[simulate_1]\n",
    "parameter: n_gene_in_block = 30\n",
    "input: genotype_file\n",
    "output: data = f\"{cwd:a}/{_input:bn}.cleaned.gz\", \n",
    "        boundary = f\"{cwd:a}/{_input:bn}_b{n_gene_in_block}.block_index.csv\", \n",
    "        original_boundary = f\"{cwd:a}/{_input:bn}_b{n_gene_in_block}.block_index_original.csv\"\n",
    "sos_run('genome_partition', input_file = _input, output_files = _output, n_gene_in_block = n_gene_in_block, col_index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[simulate_2]\n",
    "input: genotype_file, output_from('simulate_1')['original_boundary']\n",
    "output: data_partitioned = f\"{_input[1]:nn}.gz\", \n",
    "        gene_names = f\"{_input[1]:nn}.gene_names.gz\"\n",
    "python: expand = '${ }'\n",
    "    import pandas as pd\n",
    "    from collections import OrderedDict\n",
    "    data = pd.read_csv(${_input[0]:r}, compression = \"gzip\", header = 0, sep = \"\\t\", low_memory = False)\n",
    "    genes_dict = OrderedDict([(x, y) for x,y in zip([i for i in range(data.shape[1])], list(data.columns))])\n",
    "    data.columns = [i for i in range(data.shape[1])]\n",
    "    data = data.set_index([[i for i in range(data.shape[0])]])\n",
    "    bound = pd.read_csv(${_input[1]:r}, header = None, sep = \"\\t\")\n",
    "    bound2 = [[item[0], item[1]] if item[0] == bound.values[-1][0] else [item[0], bound.values[j+1][0]-1] for j, item in enumerate(bound.values)]\n",
    "    genes_fill = [genes_dict[j] for item in bound2 for j in range(item[0],item[1]+1)]\n",
    "    ## \n",
    "    pd.DataFrame(genes_fill).T.to_csv(${_output[1]:r}, compression = \"gzip\", sep = \"\\t\", header = False, index = False)\n",
    "    fill = list()\n",
    "    for l in range(data.shape[0]):\n",
    "        fill.append([data.loc[l, k[0]:k[1]].tolist() for k in bound2])\n",
    "    res = pd.DataFrame(fill)\n",
    "    # save original data by partitions\n",
    "    res.to_csv(${_output[0]:r}, compression = \"gzip\", sep = \"\\t\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Simulate effect sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[simulate_3]\n",
    "parameter: shape = 1.4\n",
    "parameter: scale = 0.6\n",
    "parameter: beta_method = \"normal\"\n",
    "parameter: pi0 = 0.95\n",
    "parameter: seed = 999\n",
    "input: output_from('simulate_1')['original_boundary']\n",
    "output: f\"{_input:nn}.{name}.beta\"\n",
    "python: expand = '${ }', stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    import pandas as pd, numpy as np\n",
    "    bound = pd.read_csv(${_input:r}, header = None, sep = \"\\t\")\n",
    "    bound2 = [[item[0], item[1]] if item[0] == bound.values[-1][0] else [item[0], bound.values[j+1][0]-1] for j, item in enumerate(bound.values)]\n",
    "    def logor_gamma(shape, scale, n):\n",
    "        return np.log(np.random.gamma(shape, scale, n))\n",
    "    def logor_normal(mean, se, n):\n",
    "        return np.random.normal(mean, se, n)\n",
    "    np.random.seed(${seed})\n",
    "    beta0 = np.log(${prevalence} / (1-${prevalence}))\n",
    "    beta1s = [x for x in logor_${beta_method}(${shape}, ${scale}, bound2[-1][1] - bound2[0][0] + 1)]\n",
    "    beta1s = [np.random.binomial(1, 1-${pi0}) * i for i in beta1s]\n",
    "    pd.DataFrame(beta1s).to_csv(${_output:r}, sep = \"\\t\", header = False, index = False)\n",
    "    print(\"shape: ${shape}\\nscale: ${scale}\\ndistribution: ${beta_method}\\npi0: ${pi0}\\nseed: ${seed}\", file=open(\"${_output:n}.yml\", 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Simulate samples using copy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[simulate_4]\n",
    "parameter: sample_size = 100000 # sample size: default 100000, test: 1000\n",
    "parameter: n_batch = 200 # number of simulated sample for each job, default: 200, test: 20\n",
    "assert sample_size % n_batch == 0\n",
    "batches = [x+1 for x in range(n_batch)]\n",
    "input: output_from('simulate_2')['data_partitioned'], output_from('simulate_3'), for_each = ['batches']\n",
    "output: f\"{_input[1]:n}.batch_cache_dir/batch_{_batches}.X.gz\", \n",
    "        f\"{_input[1]:n}.batch_cache_dir/batch_{_batches}.y\",\n",
    "        f\"{_input[1]:n}.batch_cache_dir/batch_{_batches}.case_matched.X.gz\", \n",
    "        f\"{_input[1]:n}.batch_cache_dir/batch_{_batches}.case_matched.y\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '10m', mem = '8G', cores = 1, tags = f'{step_name}_{_output[0]:bn}'\n",
    "python: expand = '${ }'\n",
    "    import pandas as pd, numpy as np\n",
    "    import random, itertools, ast\n",
    "    data = pd.read_csv(${_input[0]:r}, compression = \"gzip\", header = None, sep = \"\\t\")\n",
    "    size = int(${sample_size} / ${n_batch})\n",
    "    random.seed(${_batches})\n",
    "    samples_genome = list()\n",
    "    for i in range(size):\n",
    "        order = random.sample(data.index.tolist(), data.shape[1])\n",
    "        s = list(itertools.chain(*list(ast.literal_eval(n) for n in np.diag(data.loc[order, :]))))\n",
    "        samples_genome.append(s)\n",
    "    samples_genome_df = pd.DataFrame(samples_genome) # row: sample, column: gene\n",
    "    samples_genome_df.to_csv(${_output[0]:r}, compression = \"gzip\", sep = \"\\t\", header = False, index = False)\n",
    "    beta1s = pd.read_csv(${_input[1]:r}, header = None, sep = \"\\t\")\n",
    "    beta0 = np.log(${prevalence} / (1-${prevalence}))\n",
    "    logit_y = np.matmul(samples_genome_df.values, beta1s.values) + beta0\n",
    "    ys_p = np.exp(logit_y) / (1+np.exp(logit_y))\n",
    "    ys = np.random.binomial(1, ys_p)\n",
    "    pd.DataFrame(ys).to_csv(${_output[1]:r}, sep = \"\\t\", header = False, index = False)\n",
    "    indices1 = [i for i,y in enumerate(ys) if y == 1]\n",
    "    indices0 = np.random.choice([i for i in range(len(ys)) if i not in indices1], size = len(indices1), replace = False).tolist()\n",
    "    samples_genome_df.iloc[indices1 + indices0, :].to_csv(${_output[2]:r}, compression = \"gzip\", sep = \"\\t\", header = False, index = False)\n",
    "    pd.DataFrame([1] * len(indices1) + [0] * len(indices0)).to_csv(${_output[3]:r}, sep = \"\\t\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[simulate_5]\n",
    "input: group_by = \"all\"\n",
    "output: genotype = f'{_input[0]:dn}.X.unnamed.gz', phenotype = f'{_input[0]:dn}.y.gz'\n",
    "bash: expand = \"${ }\"\n",
    "    zcat ${paths(_input[2::4])} | gzip --best > ${_output[0]}\n",
    "    cat ${paths(_input[3::4])} | gzip --best > ${_output[1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Add gene names as columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[simulate_6]\n",
    "input: output_from('simulate_5')['genotype'], output_from('simulate_2')['gene_names']\n",
    "output: f\"{_input[0]:nn}.gz\"\n",
    "bash: expand = \"${ }\"\n",
    "    zcat ${paths(_input[1])} ${paths(_input[0])} | gzip --best > ${_output}\n",
    "    rm -f ${paths(_input[0])}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.23.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
